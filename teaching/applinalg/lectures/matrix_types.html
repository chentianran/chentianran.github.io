---
layout: ilecture_only
title: Types of matrices
---

<section class="titlepage">
    <div class="titlebox">
        <h2 class="title">Types of matrices</h2>
        <!-- <p class="small">
        </p> -->
    </div>
    <p>
        Matrices can be classified into a few general types
        based on a variety of properties.
        These names allow us to talk about matrices in a more complex way.
    </p>
    <!-- <p>Tianran Chen</p> -->
    <!-- <p class="footnote2"> -->
    <!-- Department of Mathematics<br> -->
    <!-- Auburn University at Montgomery -->
    <!-- </p> -->
</section>
<section>
    <h3>Classification of matrices</h3>
    <p>
        We can classify matrices into families
        sharing certain properties,
        and we will name each family.
    </p>
    <p class="fragment">
        These names allow us to talk about matrices
        and their applications more efficiently.
    </p>
    <p class="fragment">
        Depending on the property we focus on,
        we can classify matrices differently.
        We will discuss a few ways.
    </p>
</section>
<section>
    <h3>By size</h3>
    <p>
        The most crude classification can be done
        just by looking at the sizes (dimensions) of the matrices.
    </p>
    <ul class="fragment">
        <li>
            Square matrices
        </li>
        <li>Non-square matrices (everything else)</li>
    </ul>
    <p class="fragment">
        We may want to further distinguish matrices
        with more rows than columns or more columns than rows,
        i.e.
        \begin{align*}
            &
            \begin{bmatrix}
                * & * \\
                * & * \\
                * & * \\
                * & * \\
            \end{bmatrix}
            &&\text{vs.}
            \begin{bmatrix}
                * & * & * \\
                * & * & * \\
            \end{bmatrix}
        \end{align*}
        but there are no good names for them.
    </p>
</section>
<section>
    <h3>By sparsity</h3>
    <div class="cols">
        <div class="col">
            <p>
                We also say a matrix is <em class="highlight">sparse</em>
                if it contains "many" zero entries.
            </p>
            <p class="fragment">
                The threshold (how many is "many") is inherently vague
                and potentially subjective,
                but it is nonetheless a very useful concept in computation.
                E.g.,
                \[
                    \begin{bmatrix}
                        3 & 0 & 0 & 0 \\
                        0 & 0 & 2 & 0 \\
                        0 & 0 & 0 & 0 \\
                    \end{bmatrix}
                \]
                is a fairly sparse matrix.
            </p>
        </div>
        <div class="col">
            <p class="fragment">
                Similarly, a matrix with "many" nonzero entries
                are <em class="highlight">dense</em>.
            </p>
            <p class="fragment">
                The threshold is equally vague or subjective,
                yet it is just as useful.
                E.g.,
                \[
                    \begin{bmatrix}
                        3 & 2 & 1 & 0 \\
                        5 & 3 & 2 & 9 \\
                        2 & 1 & 0 & 8 \\
                    \end{bmatrix}
                \]
                is quite dense.
            </p>
        </div>
    </div>
</section>
<section>
    <h3>By rank</h3>
    <p>
        The rank of a matrix is an important numerical quantity
        that convey some very useful information.
    </p>
    <p class="fragment">
        The following matrices
        \[
            \begin{bmatrix}
                1 & 2 & 2  \\
                2 & 4 & 4 \\
                2 & 4 & 4 \\
            \end{bmatrix}
            , \quad
            \begin{bmatrix}
                1 & 2 & 0  \\
                2 & 4 & 5 \\
                2 & 4 & 1 \\
            \end{bmatrix}
            , \quad
            \begin{bmatrix}
                1 & 2 & 3 \\
                4 & 5 & 6 \\
                7 & 8 & 0 \\
            \end{bmatrix}
        \]
        are of rank 1, 2, and 3.
    </p>
</section>
<section>
    <h3>Zero</h3>
    <p>
        A zero matrix is a matrix consists of only zero entries.
        E.g.
        \[  
            \begin{bmatrix}
                0 & 0 & 0      \\
                0 & 0 & 0
            \end{bmatrix}.
        \]
    </p>
    <p class="fragment">
        In general, a zero matrix is
        <span class="lowlight">(dimension usually clear from context)</span>
        \[
            \mathbf{0} =
            \begin{bmatrix}
                0      & \cdots & 0      \\
                \vdots & \ddots & \vdots \\
                0      & \cdots & 0
            \end{bmatrix}.
        \]
    </p>
    <div class="cols">
        <div class="col">
            <p class="fragment">
                Clearly,
                \[
                    \mathbf{0} + A = A + \mathbf{0} = A
                \]
                So it really behaves like the number "0"
                in the world of matrices.
            </p>
        </div>
        <div class="col">
            <p class="fragment">
                Similarly, as long as dimensions are matched correctly,
                \[
                    \mathbf{0} A = \mathbf{0}
                \]
                (Note that the two zero matrices may not have the same dimension)
            </p>
        </div>
    </div>
</section>
<section>
    <h3>Square matrices</h3>
    <p>
        In the family of square matrices have the most detailed and interesting 
        classifications.
        This is partly due to square matrices represents
        functions from $\mathbb{R}^n$ to $\mathbb{R}^n$ itself.
    </p>
    <p>
        All the classifications in the following slides
        focus on square matrices.
    </p>
</section>
<section>
    <h3>Identity</h3>
    <p>
        As we have seen previously,
        the $n \times n$ <b class="highlight">identity matrix</b>,
        denoted $I_n$, is the matrix
        \[
            I_n = 
            \begin{bmatrix}
                1 &        & \\
                & \ddots & \\
                &        & 1
            \end{bmatrix}.
        \]
        (missing entries are $0$'s)
    </p>
    <p class="fragment">
        That is, it has $1$'s on the main diagonal and $0$'s elsewhere.
    </p>
    <p class="fragment">
        Whenever the dimension is clear from the context,
        we simply use $I = I_n$,
        an it is always assumed to be square.
    </p>
    <p class="fragment">
        It has the very special property that
        \[
            IA = A
        \]
        for any matrix $A$ as long as their dimensions match correctly.
    </p>
</section>
<section>
    <h3>Diagonal</h3>
    <p>
        A square matrix $A$ is said to be <b class="highlight">diagonal</b>
        if all its nonzero entries are on the main diagonal
        (upper left corner to lower right corner).
        E.g.,
        \[
            \begin{bmatrix}
                3 & 0 & 0 \\
                0 & 4 & 0 \\
                0 & 0 & 8
            \end{bmatrix}
        \]
        is a $3 \times 3$ diagonal matrix.
    </p>
    <div class="cols">
        <div class="col">
            <p class="fragment">
                It is okay for some diagonal entries to also be zero,
                e.g.,
                \[
                    \begin{bmatrix}
                        3 & 0 & 0 \\
                        0 & 0 & 0 \\
                        0 & 0 & 5
                    \end{bmatrix}
                \]
                as we only require nonzero entries to be on the main diagonal.
            </p>
        </div>
        <div class="col">
            <p class="fragment">
                In general, we can write an $n \times n$ diagonal matrix as
                \[
                    \begin{bmatrix}
                        d_1 & & \\
                            & \ddots & \\
                            &        & d_n
                    \end{bmatrix}
                \]
            </p>
        </div>
    </div>
</section>
<section>
    <h3>Triangular</h3>
    <p>
        A square matrix $A$ is said to be <b class="highlight">lower triangular</b>
        if all its nonzero entries are on or below the main diagonal
        E.g.,
        \[
            \begin{bmatrix}
                3 & 0 & 0 \\
                2 & 4 & 0 \\
                1 & 6 & 8 \\
            \end{bmatrix}
        \]
        is a $3 \times 3$ lower triangular matrix.
    </p>
    <p class="fragment">
        Similarly,
        \[
            \begin{bmatrix}
                3 & 2 & 1 \\
                0 & 8 & 7 \\
                0 & 0 & 6 \\
            \end{bmatrix}
        \]
        is an <em>upper triangular</em> matrix.
    </p>
</section>
<section>
    <h3>Singular vs nonsingular</h3>
    <p>
        This is an important classification.
        See our previous discussion for detail.
    </p>
</section>
<!-- <section>
    <h3>Unimodular</h3>
</section> -->
<section>
    <h3>Symmetric</h3>
    <div class="definition">
        A square matrix $A$ (with real entries) is
        a <b class="highlight">symmetric matrix</b> if
        \[
            A^\top = A
        \]
    </div>
    <p class="fragment">
        That is, a symmetric matrix remain unchanged if we take the transpose.
    </p>
    <p class="fragment">
        Examples:
        \begin{align*}
            A &=
            \begin{bmatrix}
                3 & 1 \\
                1 & 5
            \end{bmatrix}
            &
            B &=
            \begin{bmatrix}
                3 & 1 & 9 \\
                1 & 5 & 0 \\
                9 & 0 & 6
            \end{bmatrix}
            &
            C &=
            \begin{bmatrix}
                0 & 0 & 9 \\
                0 & 0 & 0 \\
                9 & 0 & 0
            \end{bmatrix}
        \end{align*}
        are all symmetric matrices.
    </p>
    <p class="fragment">
        Of course, square zero matrices, identity matrices, or diagonal matrices in general
        are all naturally symmetric.
    </p>
</section>
<section>
    <h3>Creating symmetric matrices</h3>
    <p>
        Starting from any matrix $A$ (not necessary square),
        we can create square <em>symmetric</em> matrices
        \begin{align*}
            & A^\top A
            &&\text{and} &
            &AA^\top
        \end{align*}
    </p>
    <div class="problem fragment">
        Verify that both of these matrices are indeed symmetric.
    </div>
    <p class="fragment">
        The transformation $A \mapsto A^\top A$ (or $A A^\top$)
        is a useful transformation used in many place.
        <span class="fragment">
            For example, we will see this in the construction of
            <em>covariance matrix</em> in statistics/data analysis
            or the <em>normal equation</em> in optimization.
        </span>
    </p>
</section>
<section>
    <h3>Variations of symmetric matrices</h3>
    <p>
        There are also quite a few variations on the property of being "symmetric".
    </p>
    <ul>
        <li class="fragment">
            A matrix $A$ is said to be <em class="highlight">skew-symmetric</em>
            if $A^\top = -A$.
            E.g.,
            \[
                \begin{bmatrix}
                    3 & -2 \\
                    2 &  4
                \end{bmatrix}
            \]
        </li>
        <li class="fragment">
            A matrix $A$ with complex entries is said to be <em class="highlight">Hermition</em>
            if $\bar{A}^\top = A$.
            Here, the "bar" notation is the complex conjugation operation.
        </li>
    </ul>
</section>
<section>
    <h3>Orthogonal</h3>
    <div class="definition">
        A square matrix $Q$ (with real entries) is
        an <b class="highlight">orthogonal matrix</b>
        (a.k.a. <b class="highlight">orthonormal matrix</b>) if
        \[
            Q^\top Q = Q Q^\top = I
        \]
    </div>
    <p class="fragment">
        Basically, the columns (or rows) of an orthogonal matrix are unit vectors
        that are orthogonal to one another
        (have zero dot product with one another).
    </p>
    <div class="cols">
        <div class="col">
            <p class="fragment">
                From the above equation,
                we can see that
                \[
                    Q^{-1} = Q^\top.
                \]
                That is, an orthogonal matrix is always invertible,
                and its transpose is also its inverse.
            </p>
        </div>
        <div class="col">
            <p class="fragment">
                We can also describe this the point of view of basis:
                the column (or rows) of an orthogonal $n \times n$ matrix
                form an orthonormal basis for $\mathbb{R}^n$.
                <span class="fragment">
                    This is usually how orthogonal matrices are created.
                </span>
            </p>
        </div>
    </div>
</section>
<section>
    <div class="problem">
        Let $Q$ be an $n \times n$ orthogonal matrix.
        Show the linear function
        \[
            f (\mathbf{x}) = Q \mathbf{x}
        \]
        preserves vector norm and angle between vectors.
    </div>
</section>
